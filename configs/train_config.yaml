model_name: "openai-community/gpt2"
dataset_name: "gsm8k"
use_cot: False
batch_size: 32
max_length: 1024 # max length for gmsk8 with cot is 538
lr: 0.0001
num_warmup_steps: 100
num_decay_steps: 500
deq_max_steps: 0
phantom_steps: 1
damp: 1.0
solver: fixed_point_iter
trainer_args:
  max_epochs: 5
  precision: bf16-mixed
  gradient_clip_val: 2.0
  accumulate_grad_batches: 1
  # devices: 2
  # strategy: "deepspeed_stage_1"
