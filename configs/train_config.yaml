model_name: "HuggingFaceTB/SmolLM2-135M"
dataset_name: "gsm8k"
use_cot: False
batch_size: 32
max_length: 1024 # max length for gmsk8 with cot is 538
lr: 0.001
num_warmup_steps: 100
num_decay_steps: 200
deq_max_steps: 4
phantom_steps: 1
damp: 0.8
trainer_args:
  max_epochs: 5
  precision: bf16-true
  gradient_clip_val: 2.0
  accumulate_grad_batches: 1
  # devices: 2
  # strategy: "deepspeed_stage_1"
