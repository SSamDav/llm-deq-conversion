model_name: "openai-community/gpt2" # "HuggingFaceTB/SmolLM2-135M"
dataset_name: "gsm8k"
use_cot: False
batch_size: 64
max_length: 1024 # max length for gmsk8 with cot is 538
lr: 0.0001
num_warmup_steps: 100
num_decay_steps: 500
deq_max_steps: 0
phantom_steps: 1
# use_adapter: True
damp: 1.0
solver: fixed_point_iter
return_final: False
trainer_args:
  max_epochs: 50
  precision: bf16-mixed
  gradient_clip_val: 2.0
  accumulate_grad_batches: 2
  # devices: 2
  # strategy: "deepspeed_stage_1"
