model_name: "HuggingFaceTB/SmolLM2-135M"
dataset_name: "gsm8k"
use_cot: False
batch_size: 128
max_length: 1024 # max length for gmsk8 with cot is 538
lr: 0.0001
num_warmup_steps: 100
num_decay_steps: 500
deq_max_steps: 4
phantom_steps: 1
damp: 0.8
solver: fixed_point_iter
trainer_args:
  max_epochs: 1
  precision: bf16-mixed
  gradient_clip_val: 2.0
  accumulate_grad_batches: 1
  # devices: 2
  # strategy: "deepspeed_stage_1"
